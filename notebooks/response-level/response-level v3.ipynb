{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e065e4e9",
   "metadata": {},
   "source": [
    "# response-level v3.ipynb\n",
    "## Author: Elliot Pallister\n",
    "\n",
    "AIM: DEVELOPING A FULLY FORMULATED RESPONSE LEVEL FEATURE VECTOR FOR VISp IN RESPONSE TO im036_r FOR SESSION ID: 1044385384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97b3447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/allensdk/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Collecting necessary imports\n",
    "\n",
    "# External imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Internal imports\n",
    "from pareto.data_io import get_cache, get_session, get_unit_channels, get_spike_times, get_stimulus_presentations, get_units_by_area, get_trials\n",
    "from pareto.preprocessing import make_psth_cube, get_image_trials, arrange_image_onsets_to_trial, group_stims_by_frame_index\n",
    "from pareto.stats import visual_selectivity_filter, subtract_baseline, zscore, roc_analysis\n",
    "from pareto.plotting import plot_pop_mean, mean_variance_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc846ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/allensdk/lib/python3.11/site-packages/hdmf/spec/namespace.py:583: UserWarning: Ignoring the following cached namespace(s) because another version is already loaded:\n",
      "core - cached version: 2.6.0-alpha, loaded version: 2.7.0\n",
      "The loaded extension(s) may not be compatible with the cached extension(s) in the file. Please check the extension documentation and ignore this warning if these versions are compatible.\n",
      "  self.warn_for_ignored_namespaces(ignored_namespaces)\n"
     ]
    }
   ],
   "source": [
    "# Importing the cache from AllenSDK\n",
    "cache = get_cache()\n",
    "\n",
    "# Using session ID 1044385384\n",
    "session_id = 1044385384\n",
    "session = get_session(session_id)\n",
    "\n",
    "units = get_unit_channels(session)\n",
    "trials = get_trials(session)\n",
    "stimulus_presentations = get_stimulus_presentations(session)\n",
    "spike_times = get_spike_times(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73cf3b",
   "metadata": {},
   "source": [
    "Firstly, I want to filter my units based on:\n",
    "\n",
    "1. Quality metrics (SNR, interspike interval violations and firing rate)\n",
    "2. Area (starting with VISp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a5b4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered units in ['VISp']: 84\n"
     ]
    }
   ],
   "source": [
    "quality_unit_filter = ((units['snr'] >= 1) & (units['isi_violations'] < 1) & (units['firing_rate'] > 0.1))\n",
    "quality_units = units.loc[quality_unit_filter].copy()\n",
    "\n",
    "area_of_interest = ['VISp']\n",
    "area_units = get_units_by_area(quality_units, area_of_interest)\n",
    "\n",
    "print(f'Number of filtered units in {area_of_interest}: {area_units.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e92837",
   "metadata": {},
   "source": [
    "Next, I want to find the stimulus onsets for the image with id: im036_r.\n",
    "\n",
    "I will use these onsets to:\n",
    "\n",
    "1. Statistically test the unit responses for selectivity to the image, calculating effect sizes and p values, using a Wilcoxon paired rank test\n",
    "2. Update the area units dataframe to contain effect sizes and p values in a column\n",
    "3. Filter the area units by selectivity for the stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321c3586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of units selective for im036_r in ['VISp']: 54\n"
     ]
    }
   ],
   "source": [
    "stim_of_interest = 'im036_r'\n",
    "stim_onsets = stimulus_presentations[stimulus_presentations['image_name'] == stim_of_interest]['start_time'].values\n",
    "\n",
    "# Define the time before the image and the duration of the window during which spikes are counts\n",
    "time_before_stim = 0.25\n",
    "duration = 0.5\n",
    "\n",
    "# Statistical testing\n",
    "selectivity_mask, effects, p_values = visual_selectivity_filter(area_units, spike_times, stim_onsets, time_before_stim, duration)\n",
    "\n",
    "area_units = area_units.copy()\n",
    "\n",
    "# Assigning effect size and p_values\n",
    "area_units.loc[:, 'p_values'] = p_values\n",
    "area_units.loc[:, 'effect_size'] = effects\n",
    "\n",
    "visual_area_units = area_units[selectivity_mask]\n",
    "\n",
    "print(f'Number of units selective for {stim_of_interest} in {area_of_interest}: {visual_area_units.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db39afd3",
   "metadata": {},
   "source": [
    "Next I want to produce a cube of dimensions U x O x T where U is the unit, O is the onset time and T is the time bin (50ms) within a set window around stimulus onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cda480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3\n",
      "(54, 1064, 75)\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 1 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 1 ... 0 0 0]\n",
      "  [0 0 0 ... 1 1 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 1 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 1 ... 0 0 0]\n",
      "  [0 0 0 ... 0 1 0]\n",
      "  [1 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "pre_onset_post_end = (0.25, 0, 0.25, 0.5)\n",
    "\n",
    "cube, unit_ids, bins = make_psth_cube(visual_area_units, spike_times, stim_onsets, pre_onset_post_end, bin_size=0.01)\n",
    "bins = bins[:-1]\n",
    "\n",
    "print(isinstance(cube, np.ndarray))\n",
    "print(cube.ndim)\n",
    "print(cube.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f681f5",
   "metadata": {},
   "source": [
    "Now that I have all of the individual data points, I will begin by baseline subtracting and z-scoring each of these traces and extract some information out by plotting a few important parameters. Here are the following questions I am looking to answer:\n",
    "\n",
    "1. What is the distribution of mean firing rate taken across all stimulus presentations across all units?\n",
    "2. What is the variance of firing rate and what is the distribution of this across the population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1626e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 25]\n",
      "[0.06484962 0.06578947 0.07236842 0.06578947 0.1343985  0.1268797\n",
      " 0.1287594  0.15601504 0.13815789 0.12406015 0.10902256 0.09116541\n",
      " 0.08270677 0.08364662 0.1156015  0.09492481 0.10526316 0.10526316\n",
      " 0.12969925 0.10244361 0.1156015  0.1137218  0.1137218  0.09210526\n",
      " 0.09210526]\n"
     ]
    }
   ],
   "source": [
    "baseline_window = [-0.25, 0]\n",
    "baseline_window = np.array(baseline_window)\n",
    "\n",
    "bsub = subtract_baseline(cube, baseline_window, bins)\n",
    "\n",
    "zs, mu, std = zscore(bsub)\n",
    "mu = np.squeeze(mu)\n",
    "std = np.squeeze(std)\n",
    "var = std ** 2\n",
    "\n",
    "\n",
    "evoked_window = np.array([0, 0.25])\n",
    "evoked_indices = np.searchsorted(bins, evoked_window)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ed31a",
   "metadata": {},
   "source": [
    "My next aim is to now build a feature vector for each unit and store this in a Pandas dataframe. Each vector will contain the following metrics:\n",
    "\n",
    "1. Mean evoked response\n",
    "2. Variance of evoked response\n",
    "3. SNR\n",
    "5. Mean AUC for trial chunks compared to first (ROC ANALYIS)\n",
    "6. Mean AUC for consecutive trial chunks (ROC ANALYSIS)\n",
    "7. Tuning based selectivity index\n",
    "8. Population sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a53c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7134546830438268, 0.16715315054612778, 0.4020209164778138, 0.07256120498678784, 0.14342672818547325, 0.44664493474076616, 0.21954291147060423, 0.015101873905110919, 0.08408074438453143, 0.26168865216300263, -0.2879116606122853, 0.44723546932260777, 0.09801230327650937, 0.18507856625433655, -0.2646023634525721, 0.3606902431366869, 0.006824002729944793, 0.26205895680966507, 0.23018390257684826, 0.3975854153578544, 0.3943405431185601, -0.16259208451933063, 0.2809619987616095, 1.04319962899487, -0.10966952147332083, 0.25930606625367536, 0.374288335228282, 0.6906463064927258, 0.6956907457834272, 0.5229058058990597, 0.41083531185435934, 0.1835921647473505, 0.9224917802163763, -0.01875032441644379, 0.25013717422157017, 0.3485588035285266, 0.3549246017468689, -0.040355116277904765, 0.8392384114777222, 0.7568303874415098, -0.04042065017220026, 0.5295144358191571, -0.07646791870860466, 0.13570751369585904, 0.22128400223081443, 0.31422925849732875, 0.4364708577362672, 1.130966032663564, 1.0613263561371624, 0.44152856095447307, 0.4956352939680023, 0.3491528279980716, 0.17748638190226632, 0.38236052359162903]\n",
      "(54,)\n",
      "The population sparsity for im036_r is 0.5884578443472565\n"
     ]
    }
   ],
   "source": [
    "def get_snr(\n",
    "    responses,\n",
    "    baseline_window, \n",
    "    evoked_window,\n",
    "    bins\n",
    "    ):\n",
    "\n",
    "  bi = np.searchsorted(bins, baseline_window)\n",
    "  ei = np.searchsorted(bins, evoked_window)\n",
    "\n",
    "  baseline = responses[:, bi[0]:bi[1]].mean(axis=1)\n",
    "  evoked = responses[:, ei[0]:ei[1]].mean(axis=1)\n",
    "\n",
    "  bsub = evoked - baseline\n",
    "  signal = bsub.mean()\n",
    "  noise = np.std(bsub)\n",
    "\n",
    "  snr = signal / noise\n",
    "\n",
    "  return snr\n",
    "\n",
    "\n",
    "snrs = []\n",
    "\n",
    "for unit_responses in cube:\n",
    "  snr = get_snr(unit_responses, baseline_window, evoked_window, bins)\n",
    "  snrs.append(snr)\n",
    "\n",
    "print(snrs)\n",
    "\n",
    "def population_sparseness(\n",
    "    responses,\n",
    "    baseline_window,\n",
    "    evoked_window,\n",
    "    bins\n",
    "    ):\n",
    "  \n",
    "  bi = np.searchsorted(bins, baseline_window)\n",
    "  ei = np.searchsorted(bins, evoked_window)\n",
    "\n",
    "  baseline = responses[:, :, bi[0]:bi[1]].mean(axis=1)\n",
    "  evoked = responses[:, :, ei[0]:ei[1]].mean(axis=1)\n",
    "\n",
    "  bsub = evoked - baseline\n",
    "\n",
    "  mu_evoked = bsub.mean(axis=1)\n",
    "\n",
    "  print(mu_evoked.shape)\n",
    "\n",
    "  num = (1 - ((mu_evoked.mean() ** 2)/((mu_evoked ** 2).mean())))\n",
    "  den = (1 - (1 / mu_evoked.shape[0]))\n",
    "\n",
    "  sparsity = num / den\n",
    "\n",
    "  return sparsity\n",
    "\n",
    "sparsity = population_sparseness(cube, baseline_window, evoked_window, bins)\n",
    "\n",
    "print(f\"The population sparsity for {stim_of_interest} is {sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85870a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n",
      "[ 0.04432331  0.0112782   0.03206767  0.00225564  0.00353383  0.03052632\n",
      "  0.00759398  0.00142857  0.01451128  0.01721804 -0.01827068  0.02439849\n",
      "  0.00394737  0.01003759 -0.0162782   0.02240601  0.00026316  0.01180451\n",
      "  0.01304511  0.01958646  0.01793233 -0.00943609  0.01233083  0.08943609\n",
      " -0.00503759  0.02        0.01774436  0.05469925  0.03740601  0.03898496\n",
      "  0.0181203   0.01278195  0.05578947 -0.00109023  0.015       0.02218045\n",
      "  0.03890977 -0.0024812   0.05586466  0.03041353 -0.00195489  0.04699248\n",
      " -0.00421053  0.00394737  0.00522556  0.01672932  0.01721804  0.10924811\n",
      "  0.09233083  0.02849624  0.03018797  0.01883459  0.00548872  0.03578947]\n",
      "[0.0038595  0.0045525  0.00636264 0.00096634 0.00060706 0.00467115\n",
      " 0.00119647 0.00894834 0.02978642 0.0043291  0.00402708 0.00297614\n",
      " 0.00162201 0.00294135 0.00378464 0.00385887 0.00148715 0.00202907\n",
      " 0.00321178 0.0024269  0.0020679  0.0033681  0.00192615 0.00735006\n",
      " 0.00210996 0.00594887 0.00224754 0.00627265 0.00289102 0.00555837\n",
      " 0.00194534 0.00484715 0.00365746 0.00338077 0.00359605 0.00404938\n",
      " 0.01201836 0.00378031 0.00443102 0.00161487 0.00233904 0.00787592\n",
      " 0.0030319  0.00084607 0.00055766 0.00283442 0.00155617 0.00933101\n",
      " 0.00756825 0.00416541 0.00370974 0.00290992 0.00095634 0.00876122]\n"
     ]
    }
   ],
   "source": [
    "def auc_roc(responses, chunk_size):\n",
    "\n",
    "  n_chunks = int(round(responses.shape[1]/chunk_size))\n",
    "\n",
    "  unit_chunks = []\n",
    "\n",
    "  for unit in responses:\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "      chunk = unit[i*20:(i+1)*20]\n",
    "      chunks.append(chunk)\n",
    "  \n",
    "    chunks = np.array(chunks)\n",
    "    unit_chunks.append(chunks)\n",
    "\n",
    "  unit_chunks = np.array(unit_chunks)\n",
    "\n",
    "  unit_TPRs, unit_FPRs, unit_AUCs = [], [], []\n",
    "\n",
    "  for unit in unit_chunks:\n",
    "\n",
    "    TPRs, FPRs, AUCs = [], [], []\n",
    "\n",
    "    for c in range(n_chunks-1):\n",
    "\n",
    "      TPR, FPR = roc_analysis(unit[0], unit[c+1])\n",
    "      TPR, FPR = np.array(TPR), np.array(FPR)\n",
    "      TPRs.append(TPR)\n",
    "      FPRs.append(FPR)\n",
    "      order = np.argsort(FPR)\n",
    "      auc = np.trapz(TPR[order], FPR[order]) \n",
    "      if auc < 0.5:\n",
    "        auc = 1 - auc\n",
    "      AUCs.append(auc)\n",
    "\n",
    "    TPRs, FPRs, AUCs = np.array(TPRs), np.array(FPRs), np.array(AUCs)\n",
    "    unit_TPRs.append(TPRs)\n",
    "    unit_FPRs.append(FPRs)\n",
    "    unit_AUCs.append(AUCs)\n",
    "\n",
    "  unit_TPRs, unit_FPRs, unit_AUCs = np.array(unit_TPRs), np.array(unit_FPRs), np.array(unit_AUCs)\n",
    "\n",
    "  return unit_TPRs, unit_FPRs, unit_AUCs\n",
    "\n",
    "evoked_responses = bsub[:,:,evoked_indices[0]:evoked_indices[1]].mean(axis=2)\n",
    "\n",
    "TPRs, FPRs, AUCs = auc_roc(evoked_responses, 20)\n",
    "\n",
    "print(len(AUCs))\n",
    "print(len(snrs))\n",
    "\n",
    "means = evoked_responses.mean(axis=1)\n",
    "variances = evoked_responses.var(axis=1)\n",
    "\n",
    "print(means)\n",
    "print(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_evoked  evoked_variance       snr  evoked_AUC_compared_to_first\n",
      "0     0.044323         0.003860  0.713455                      0.588534\n",
      "1     0.011278         0.004553  0.167153                      0.696274\n",
      "2     0.032068         0.006363  0.402021                      0.658654\n",
      "3     0.002256         0.000966  0.072561                      0.599183\n",
      "4     0.003534         0.000607  0.143427                      0.540240\n",
      "0.10924811\n"
     ]
    }
   ],
   "source": [
    "mean_AUCs = AUCs.mean(axis=1)\n",
    "\n",
    "unit_features = pd.DataFrame(columns=['mean_evoked', 'evoked_variance', 'snr', 'evoked_AUC_compared_to_first'])\n",
    "\n",
    "unit_features['mean_evoked'] = means\n",
    "unit_features['evoked_variance'] = variances\n",
    "unit_features['snr'] = snrs\n",
    "unit_features['evoked_AUC_compared_to_first'] = mean_AUCs\n",
    "\n",
    "print(unit_features.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allensdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
